{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8369, 36)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "balance_df = pd.read_excel('balance_frame.xlsx')\n",
    "print(balance_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Gear', 'Year of Manufacture', 'modelYear', 'km', 'transmission','Mileage',\n",
    "            'City','bt','ownerNo','Insurance Validity','Fuel Type',]#'model''Mileage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7697, 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outliers removal using IQR\n",
    "\n",
    "Q1 = balance_df.quantile(0.05)\n",
    "Q3 = balance_df.quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "balance_df = balance_df[~((balance_df < (Q1 - 1.5 * IQR)) |(balance_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "balance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gearbox(gear_box):\n",
    "    if gear_box in [0, 1, 2, 3]:\n",
    "        return 4\n",
    "    return gear_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gear\n",
       "4    6945\n",
       "5     379\n",
       "6     263\n",
       "7     110\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df['Gear'] = balance_df['Gear'].apply(clean_gearbox)\n",
    "balance_df['Gear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# X_train[features] = scaler.fit_transform(X_train[features])\n",
    "# X_test[features] = scaler.fit_transform(X_test[features])\n",
    "balance_df[features] = scaler.fit_transform(balance_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = balance_df[features]\n",
    "y = balance_df['price_in_lakhs']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6157, 11), (6157,), (1540, 11), (1540,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 400],  # More granularity for tree count\n",
    "    'max_depth': [5, 10, 15, 20, None],  # Including deeper trees and no limit\n",
    "    'min_samples_split': [2, 5, 10, 20],  # Explore stricter splitting rules\n",
    "    'min_samples_leaf': [1, 2, 4, 8],  # Experiment with leaf size\n",
    "    'bootstrap': [True, False],  # Test both bootstrapping and non-bootstrapping\n",
    "    'criterion': ['squared_error', 'absolute_error', 'poisson'],  # Include Poisson criterion\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Explore feature subsets\n",
    "    'oob_score': [True, False],  # Out-of-bag scoring for more robust validation\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=parameters, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model with best parameters\n",
    "model = RandomForestRegressor(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'RMSE : {rmse}')\n",
    "print(f'MAE : {mae}')\n",
    "print(f'R2 Score : {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(model,open('random_forest_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"First experiment\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric('rmse',rmse)\n",
    "    mlflow.log_metric('mae',mae)\n",
    "    mlflow.log_metric('r2',r2)\n",
    "\n",
    "    mlflow.sklearn.log_model(model,\"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "models = [\n",
    "    (\n",
    "        \"Gradient Boosting Regressor\",\n",
    "        \n",
    "        {\"n_estimators\": 150, \"learning_rate\": 0.1},\n",
    "        GradientBoostingRegressor(),  # Example of hyperparameters\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        {\"n_estimators\": 150, \"max_depth\": 5}, \n",
    "         RandomForestRegressor(), # Corrected position of params\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Support Vector Regression\",\n",
    "        \n",
    "        {\"kernel\": \"rbf\", \"C\": 1.0, \"epsilon\": 0.1}, \n",
    "         SVR(), # Params for SVR\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGB Regressor\",\n",
    "       \n",
    "        {\"n_estimators\": 100, \"learning_rate\": 0.1}, \n",
    "        XGBRegressor(), # Params for XGBoost\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = min(len(X), len(y))\n",
    "X = X[:min_samples]\n",
    "y = y[:min_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check consistency of data\n",
    "assert len(X) == len(y), \"Features (X) and target (y) must have the same number of samples!\"\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the sizes after the split\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports=[]\n",
    "\n",
    "for model_name, params,model, train_set, test_set in models:\n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    # Apply hyperparameters and train the model\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store the results\n",
    "    reports.append((model_name, rmse, mae, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gear                   0\n",
       "Year of Manufacture    5\n",
       "modelYear              0\n",
       "km                     0\n",
       "transmission           0\n",
       "Mileage                0\n",
       "City                   0\n",
       "bt                     0\n",
       "ownerNo                0\n",
       "Insurance Validity     0\n",
       "Fuel Type              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gear                   0\n",
       "Year of Manufacture    0\n",
       "modelYear              0\n",
       "km                     0\n",
       "transmission           0\n",
       "Mileage                0\n",
       "City                   0\n",
       "bt                     0\n",
       "ownerNo                0\n",
       "Insurance Validity     0\n",
       "Fuel Type              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gradient Boosting Regressor, RMSE: 4.2559, MAE: 2.0546, R2: 0.7847\n",
      "Model: Random Forest, RMSE: 4.9590, MAE: 2.5546, R2: 0.7077\n",
      "Model: Support Vector Regression, RMSE: 4.7907, MAE: 2.1758, R2: 0.7272\n",
      "Model: XGB Regressor, RMSE: 3.8067, MAE: 1.7298, R2: 0.8278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer to handle NaN values\n",
    "imputer = SimpleImputer(strategy='mean')  # Replace NaN with column mean\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "reports = []\n",
    "\n",
    "for model_name, params, model, train_set, test_set in models:\n",
    "    # Extract training and testing sets\n",
    "    X_train, y_train = train_set\n",
    "    X_test, y_test = test_set\n",
    "\n",
    "    # Handle missing values in X_train and X_test\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "    # Debugging step: Check and align data lengths\n",
    "    min_train_samples = min(len(X_train), len(y_train))\n",
    "    X_train, y_train = X_train[:min_train_samples], y_train[:min_train_samples]\n",
    "\n",
    "    min_test_samples = min(len(X_test), len(y_test))\n",
    "    X_test, y_test = X_test[:min_test_samples], y_test[:min_test_samples]\n",
    "\n",
    "    # Apply hyperparameters and train the model\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store the results\n",
    "    reports.append((model_name, rmse, mae, r2))\n",
    "\n",
    "# Print the results\n",
    "for report in reports:\n",
    "    print(f\"Model: {report[0]}, RMSE: {report[1]:.4f}, MAE: {report[2]:.4f}, R2: {report[3]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\mlflow_car\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:11:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2024/12/22 22:11:41 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ARAVINTH\\AppData\\Local\\Temp\\tmp1ftckdco\\model, flavor: xgboost). Fall back to return ['xgboost==2.1.3']. Set logging level to DEBUG to see the full traceback. \n",
      "m:\\mlflow_car\\.venv\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The XGBRegressor or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n",
      "2024/12/22 22:11:41 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": [\n",
      "    [\n",
      "      0.0,\n",
      "      0.4761904761904816,\n",
      "      0.607142857142847,\n",
      "      0.3402730769230769,\n",
      "      1.0,\n",
      "      0.31884057971014496,\n",
      "      0.6000000000000001,\n",
      "      0.7777777777777777,\n",
      "      0.6000000000000001,\n",
      "      0.5714285714285714,\n",
      "      1.0\n",
      "    ]\n",
      "  ]\n",
      "}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: 'super' object has no attribute '__sklearn_tags__'\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]\n",
    "    report = reports[i]\n",
    "\n",
    "    # Extract the metrics for this model\n",
    "    rmse, mae, r2 = report[1], report[2], report[3]\n",
    "\n",
    "    # Create an input example as a NumPy array to ensure compatibility\n",
    "    input_example = np.array(X_train[:1])  # Single row of the training data\n",
    "    signature = infer_signature(X_train, model.predict(X_train[:5]))  # Input and output schema\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric('rmse', rmse)\n",
    "        mlflow.log_metric('mae', mae)\n",
    "        mlflow.log_metric('r2', r2)\n",
    "\n",
    "        # Log the model with input example and signature\n",
    "        if 'XGB' in model_name:\n",
    "            mlflow.xgboost.log_model(model, \"model\", signature=signature, input_example=input_example)\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\", signature=signature, input_example=input_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
